Chapter 1
A Perspective on Testing
Why do we test? The two main reasons are to make a judgment about quality or acceptability and
to discover problems. We test because we know that we are fallible—this is especially true in the
domain of software and software-controlled systems. The goal of this chapter is to create a framework within which we can examine software testing.
1.1 Basic Definitions
Much of testing literature is mired in confusing (and sometimes inconsistent) terminology, probably because testing technology has evolved over decades and via scores of writers. The International
Software Testing Qualification Board (ISTQB) has an extensive glossary of testing terms (see the
website http://www.istqb.org/downloads/glossary.html). The terminology here (and throughout
this book) is compatible with the ISTQB definitions, and they, in turn, are compatible with the
standards developed by the Institute of Electronics and Electrical Engineers (IEEE) Computer
Society (IEEE, 1983). To get started, here is a useful progression of terms.
Error—People make errors. A good synonym is mistake. When people make mistakes while
coding, we call these mistakes bugs. Errors tend to propagate; a requirements error may be magnified during design and amplified still more during coding.
Fault—A fault is the result of an error. It is more precise to say that a fault is the representation of an error, where representation is the mode of expression, such as narrative text, Unified
Modeling Language diagrams, hierarchy charts, and source code. Defect (see the ISTQB Glossary)
is a good synonym for fault, as is bug. Faults can be elusive. An error of omission results in a fault
in which something is missing that should be present in the representation. This suggests a useful
refinement; we might speak of faults of commission and faults of omission. A fault of commission
occurs when we enter something into a representation that is incorrect. Faults of omission occur
when we fail to enter correct information. Of these two types, faults of omission are more difficult
to detect and resolve.
Failure—A failure occurs when the code corresponding to a fault executes. Two subtleties
arise here: one is that failures only occur in an executable representation, which is usually taken
to be source code, or more precisely, loaded object code; the second subtlety is that this definition 

Software Testing
© 2010 Taylor & Francis Group, LLC
relates failures only to faults of commission. How can we deal with failures that correspond to
faults of omission? We can push this still further: what about faults that never happen to execute,
or perhaps do not execute for a long time? Reviews (see Chapter 22) prevent many failures by finding faults; in fact, well-done reviews can find faults of omission.
Incident—When a failure occurs, it may or may not be readily apparent to the user (or customer or tester). An incident is the symptom associated with a failure that alerts the user to the
occurrence of a failure.
Test—Testing is obviously concerned with errors, faults, failures, and incidents. A test is the
act of exercising software with test cases. A test has two distinct goals: to find failures or to demonstrate correct execution.
Test case—A test case has an identity and is associated with a program behavior. It also has a
set of inputs and expected outputs.
Figure 1.1 portrays a life cycle model for testing. Notice that, in the development phases,
three opportunities arise for errors to be made, resulting in faults that may propagate through the
remainder of the development process. The fault resolution step is another opportunity for errors
(and new faults). When a fix causes formerly correct software to misbehave, the fix is deficient. We
will revisit this when we discuss regression testing.
From this sequence of terms, we see that test cases occupy a central position in testing. The
process of testing can be subdivided into separate steps: test planning, test case development, running test cases, and evaluating test results. The focus of this book is how to identify useful sets of
test cases.
1.2  Test Cases
The essence of software testing is to determine a set of test cases for the item to be tested. A test
case is (or should be) a recognized work product. A complete test case will contain a test case identifier, a brief statement of purpose (e.g., a business rule), a description of preconditions, the actual
test case inputs, the expected outputs, a description of expected postconditions, and an execution
history. The execution history is primarily for test management use—it may contain the date when
the test was run, the person who ran it, the version on which it was run, and the pass/fail result.
Spec
Fault
Fault
Fault Incident
Design
Coding Classify
fault
Isolate
fault
Fault
resolution

A testing life cycle.

relates failures only to faults of commission. How can we deal with failures that correspond to
faults of omission? We can push this still further: what about faults that never happen to execute,
or perhaps do not execute for a long time? Reviews (see Chapter 22) prevent many failures by finding faults; in fact, well-done reviews can find faults of omission.
Incident—When a failure occurs, it may or may not be readily apparent to the user (or customer or tester). An incident is the symptom associated with a failure that alerts the user to the
occurrence of a failure.
Test—Testing is obviously concerned with errors, faults, failures, and incidents. A test is the
act of exercising software with test cases. A test has two distinct goals: to find failures or to demonstrate correct execution.
Test case—A test case has an identity and is associated with a program behavior. It also has a
set of inputs and expected outputs.
Figure 1.1 portrays a life cycle model for testing. Notice that, in the development phases,
three opportunities arise for errors to be made, resulting in faults that may propagate through the
remainder of the development process. The fault resolution step is another opportunity for errors
(and new faults). When a fix causes formerly correct software to misbehave, the fix is deficient. We
will revisit this when we discuss regression testing.
From this sequence of terms, we see that test cases occupy a central position in testing. The
process of testing can be subdivided into separate steps: test planning, test case development, running test cases, and evaluating test results. The focus of this book is how to identify useful sets of
test cases.



...........................


Deep Dive into Software Testing Methodologies:
Software testing methodologies provide frameworks for approaching and executing software testing activities. Understanding these methodologies is crucial for SQA engineers to choose the best approach for different situations and ensure comprehensive software quality. Let's explore the key types:

1. Black-Box Testing:

Focuses on the external behavior of the software from the user's perspective.
Doesn't require knowledge of internal code structure.
Techniques: Equivalence partitioning, boundary value analysis, decision tables, user stories.
Useful for functional testing, usability testing, acceptance testing.
2. White-Box Testing:

Focuses on the internal structure of the software code.
Requires knowledge of programming languages and code structure.
Techniques: Unit testing, code coverage analysis, mutation testing.
Useful for ensuring code correctness, identifying logic errors, and meeting coding standards.
3. Grey-Box Testing:

Combines elements of black-box and white-box testing.
Utilizes some knowledge of internal code structure to inform test design, but primarily focuses on user-oriented testing.
Techniques: Integration testing, API testing, security testing.
Useful for testing interactions between components, APIs, and identifying security vulnerabilities.
4. Waterfall Model:

Linear, sequential approach where testing occurs after development is complete.
Each phase (requirements, design, development, testing, deployment) is completed before moving to the next.
Suitable for well-defined, stable projects with clear requirements.
Less adaptable to changes and can be time-consuming in later stages for major fixes.
5. Agile Model:

Iterative and incremental approach where testing is integrated throughout the development cycle.
Short development sprints deliver working software features, with testing happening within each sprint.
Adaptable to changes and encourages early feedback.
Requires close collaboration between testers and developers.
6. V-Model:

Combines elements of waterfall and agile methodologies.
Testing phases mirror development phases in a V-shape (requirements verified at top, code tested at bottom).
Offers a structured approach with early validation and verification.
Can be complex to manage and less adaptable to significant changes.
7. Spiral Model:

Risk-driven approach where testing focuses on identifying and mitigating risks at each stage of development.
Iterative cycles with risk assessment, prototyping, development, and evaluation.
Suitable for complex projects with high-risk factors.
Can be resource-intensive and require experienced testers for risk evaluation.
8. DevOps Approach:

Continuous integration and continuous delivery (CI/CD) pipeline emphasizes automated testing throughout the development process.
Tests run after every code change, providing rapid feedback and early identification of issues.
Requires robust testing infrastructure and automation tools.
Promotes faster release cycles and improved software quality.
Choosing the Right Methodology:

The best testing methodology depends on various factors like project size, complexity, risk, development lifecycle, and team structure. Often, a combination of methodologies might be used to suit different testing needs within a project.

Remember, this is just an overview. Feel free to ask specific questions about any of these methodologies or delve deeper into specific techniques and tools within each approach!



